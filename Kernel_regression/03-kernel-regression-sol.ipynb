{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will predict the median value of houses in Boston using a quadratic basis and an equivalent kernel method.\n",
    "\n",
    "### Assumed knowledge\n",
    "- Linear regression (Week 2)\n",
    "- Kernel methods (Week 4; Bishop 6.1, 6.2)\n",
    "\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Applying machine learning techniques with a non-linear basis\n",
    "- Using kernel methods instead of a new basis\n",
    "- Evaluating when using a kernel method would or would not be sensible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "In this lab, we will use the same [dataset](https://machlearn.gitlab.io/sml2019/tutorials/02-dataset.csv) as in week 2, which describes the price of housing in Boston (see [description](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)). \n",
    "We aim to predict the value of a home from other factors.\n",
    "In this dataset, each row represents the data of one house. The first column is the value of the house, which is the target to predict. The remaining columns are features, which has been normalised to be in the range $[-1, 1]$. The corresponding labels of columns are\n",
    "\n",
    "```'medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'```\n",
    "\n",
    "Download the dataset. Read in the data using ```np.loadtxt``` with the optional argument ```delimiter=','```, as our data is comma separated rather than space separated. Remove the column containing the binary variable ```'chas'```.\n",
    "\n",
    "Check that the data is as expected using ```print()```. It should have 506 rows (examples) and 13 columns (1 label and 12 features). Check that this is the case. \n",
    "\n",
    "Hint: use  assert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "loaded_data = np.loadtxt('03-dataset.csv', delimiter=',')\n",
    "\n",
    "# remove chas\n",
    "column_idxes = list(range(len(names)))\n",
    "chas_idx = names.index('chas')\n",
    "wanted_columns = list(column_idxes)\n",
    "wanted_columns.remove(chas_idx)\n",
    "data = loaded_data[:,wanted_columns]\n",
    "data_names = list(names)\n",
    "data_names.remove('chas')\n",
    "\n",
    "print(data_names)\n",
    "print(np.array_str(data[:5], precision=3))\n",
    "assert data.shape == (506,13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before coding: Explain kernel methods, dual representations \n",
    "\n",
    "Please spend 5 minutes to explain to your neighbours what are kernel methods and dual representations.\n",
    "\n",
    "Then find a piece of paper, derive how to from the linear regression model with regularised sum-of-squares error \n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N(\\mathbf{w}^T \\phi(\\mathbf{x_n}) - t_n)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T\\mathbf{w},$$\n",
    "get to the dual representations \n",
    "$$J(\\mathbf{a}) = \\frac{1}{2} \\mathbf{a}^T \\mathbf{KKa} - \\mathbf{a}^T \\mathbf{Kt} + \\frac{1}{2}\\mathbf{t}^T \\mathbf{t} + \\frac{\\lambda}{2} \\mathbf{a}^T\\mathbf{Ka},$$\n",
    "where $\\mathbf{K=\\Phi \\Phi}^T$ with elements $K_{nm} = \\phi(\\mathbf{x_n})^T\\phi(\\mathbf{x_m})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains two parts. Part one is to implement linear regression with a basis function, for example, [a polynomial basis function of degree 2](https://en.wikipedia.org/wiki/Polynomial_kernel) as mentioned in week 2, we call it quadratic basis function below. The basis function can be understood as a feature mapping from raw input space into the new feature space. Part two is kernel regression, i.e. linear regression with kernel function $k(\\mathbf{x,y}) = (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2$. We'll see the kernel function implementation is equavilant of the quadratic basis function implementation.  \n",
    "\n",
    "## Refresh: Linear regression\n",
    "\n",
    "First remind yourself of what linear regression is by our implemention of linear regression with regularisation on the Boston dataset. Use 80% of the available data for training the model using maximum likelihood with regularisation (assuming Gaussian noise). The rest of the data is allocated to the test set.\n",
    "\n",
    "Report the root mean squared error (RMSE) for the training set and the test set. We'll compare the results with the linear regression with basis function, and the linear regression with kernel function later on.\n",
    "\n",
    "**TODO**: Implement the analytic solution of $\\frac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}}=0$ in the function  ```w_ml_regularised(Phi, t, l)```, where ```l``` stands for $\\lambda$. Note that $$\\Phi=\n",
    "\\begin{pmatrix}\n",
    "    \\phi(\\mathbf{x_1})^T \\\\\n",
    "    \\phi(\\mathbf{x_2})^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\phi(\\mathbf{x_n})^T \\\\\n",
    "\\end{pmatrix},$$\n",
    "where $\\phi(\\mathbf{x_i})$ denotes the feature vector for the $i$-th datapoint, and $n$ denotes the total number of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def w_ml_regularised(Phi, t, l):\n",
    "    \"\"\"Produce the analytic solution of w given input features and labels.\"\"\"\n",
    "\n",
    "    return np.linalg.inv(l * np.eye(Phi.shape[1]) + Phi.T @ Phi) @ Phi.T @ t\n",
    "\n",
    "def split_data(data, train_size):\n",
    "    \"\"\"Randomly split data into two groups. The first group is a fifth of the data.\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(train_size * N)]\n",
    "    test_idx = idx[int(train_size * N):]\n",
    "\n",
    "    # Assume label is in the first column\n",
    "    X_train = data[train_idx, 1:]\n",
    "    t_train = data[train_idx, 0]\n",
    "    X_test = data[test_idx, 1:]\n",
    "    t_test = data[test_idx, 0]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def rmse(X_train, t_train, X_test, t_test, w):\n",
    "    \"\"\"Return the RMSE for training and test sets\"\"\"\n",
    "    N_train = len(X_train)\n",
    "    N_test = len(X_test)\n",
    "\n",
    "    # Training set error\n",
    "    t_train_pred = np.dot(X_train, w)\n",
    "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
    "\n",
    "    # Test set error\n",
    "    t_test_pred = np.dot(X_test, w)\n",
    "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
    "\n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "def lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    train_rmse: float, RMSE for training set\n",
    "    test_rmse: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    X0 = np.ones((data.shape[0], 1))\n",
    "    X = np.hstack((X, X0))\n",
    "\n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    w_reg = w_ml_regularised(X_train, t_train,l)\n",
    "\n",
    "    train_rmse, test_rmse = rmse(X_train, t_train, X_test, t_test, w_reg)\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "train_rmse, test_rmse = lr(data, 1.1, 0.8)\n",
    "print(\"Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 4.74, Test 4.89\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear regression with quadratic basis function\n",
    "\n",
    "Let $X \\in \\RR^{n \\times d}$ be the data matrix containing n datapoints $\\mathbf{x_i} \\in \\RR^{d}$. We can choose to train and test using the raw data $X$ as the input to our model as the above method. Alternatively, we could use $$\\Phi= [\\mathbf{\\phi(x_1)}, \\mathbf{\\phi(x_2)}, ..., \\mathbf{\\phi(x_n)}]^T \\in \\RR^{n \\times m},$$ where $\\mathbf{\\phi(x_i)}$ is some transformation of $\\mathbf{x_i}$, m is the dimension of $\\mathbf{\\phi(x_i)}$.\n",
    "\n",
    "For this lab, write $\\mathbf{x_i} = (x_{i,1},\\dots,x_{i,d})$. Let\n",
    "$$\n",
    "\\phi(\\mathbf{x_i}) = (x_{i,1}^2, x_{i,2}^2, \\ldots, x_{i,d}^2, \\sqrt{2}x_{i,1} x_{i,2}, \\ldots, \\sqrt{2}x_{i,d-1}x_{i,d}, \\sqrt{2}x_{i,1}, \\ldots, \\sqrt{2}x_{i,d}, 1).\n",
    "$$\n",
    "Note that $\\phi(\\mathbf{x_i})$ is all quadratic functions of elements of $\\mathbf{x_i}$ and 1 (The $\\sqrt{2}$ coefficients are for normalisation for later in the lab).\n",
    "We say that we are using a *quadratic basis function*.\n",
    "\n",
    "Train and test the data with ```quadratic_lr```, report the RMSE for the training set and the test set.\n",
    "\n",
    "**TODO**:  \n",
    "1. write a function called ```phi_quadratic``` with a single datapoint $\\mathbf{x_i}$ as input and the quadratic basis function $\\phi(\\mathbf{x_i})$ as output.  \n",
    "2. write a function called ```feature_map``` with raw data matrix $X$ as input and $\\Phi$ as output by using ```phi_quadratic``` for each datapoint. \n",
    "3. in function ```quadratic_lr```, make use of previous functions and give the analytic solution for $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def phi_quadratic(x):\n",
    "    \"\"\"Compute phi(x) for a single training example using quadratic basis function.\"\"\"\n",
    "    D = len(x)\n",
    "    # Features are (1, {x_i}, {cross terms and squared terms})\n",
    "    # Cross terms x_i x_j and squared terms x_i^2 can be taken from upper triangle of outer product of x with itself\n",
    "    utri = np.sqrt(2)*np.hstack((x, np.outer(x, x)[np.triu_indices(D, k=1)]))\n",
    "    diag = x * x\n",
    "    res = np.hstack((1,diag, utri))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def feature_map(X):\n",
    "    \"\"\"Return the matrix of the feature map.\"\"\"\n",
    "    num_ex, num_feat = X.shape\n",
    "    D = int(1+num_feat+num_feat*(num_feat+1)/2)\n",
    "    Phi = np.zeros((num_ex, D))\n",
    "    np.shape(Phi)\n",
    "    for ix in range(num_ex):\n",
    "        Phi[ix,:] = phi_quadratic(X[ix,:])\n",
    "    return Phi\n",
    "\n",
    "def quadratic_lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    train_rmse: float, RMSE for training set\n",
    "    test_rmse: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    # alternatively: use train_test_split\n",
    "    # X_train, X_test, t_train, t_test = train_test_split(X[:,1:], X[:, 0], test_size = 1-split_rate, random_state = 42)\n",
    "    w_reg = w_ml_regularised(feature_map(X_train), t_train,l)\n",
    "\n",
    "    train_rmse, test_rmse = rmse(feature_map(X_train), t_train, feature_map(X_test), t_test, w_reg)\n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "train_rmse, test_rmse = quadratic_lr(data, 1.1, 0.8)\n",
    "print(\"Quadratic basis: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 2.79, Test 3.34\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear regression with a kernel\n",
    "\n",
    "### Computing a basis transformation as an inner product\n",
    "\n",
    "Define $k(\\mathbf{x,y}) = (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2$, where $\\mathbf{x,y} \\in \\mathbb{R}^2$. One way to verify $k(\\mathbf{x,y})$ is a kernel function is to write this as an inner product of a verctor valued function evaluated at $\\mathbf{x}$ and $\\mathbf{y}$. That is, show we have $k(\\mathbf{x}, \\mathbf{y}) = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{y})}$ and specify what is $\\phi(\\mathbf{x}), \\phi(\\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\\begin{align}\n",
    "k(\\mathbf{x,y}) &= (\\dotprod{\\mathbf{x}}{\\mathbf{y}} + 1)^2 = (\\mathbf{x}^T\\mathbf{y}+ 1)^2 = (x_1y_1 + x_2y_2 + 1)^2\\\\\n",
    "&= x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2 + 2x_1y_1 + 2x_2y_2 + 1\\\\\n",
    "&= (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, 1)(y_1^2, y_2^2, \\sqrt{2} y_1y_2, \\sqrt{2}y_1, \\sqrt{2}y_2, 1)^T\n",
    "\\end{align}\n",
    "Let $\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2} x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, 1)^T$, $\\phi(\\mathbf{y}) = (y_1^2, y_2^2, \\sqrt{2} y_1y_2, \\sqrt{2}y_1, \\sqrt{2}y_2, 1)^T$, then we have $k(\\mathbf{x,y}) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{y}) = \\dotprod{\\phi(\\mathbf{x})}{\\phi(\\mathbf{y})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convince yourself that, for $X \\in \\RR^{n \\times d}, Y \\in \\RR^{m \\times d}$, then $K = (X Y^T  + 1)^2 \\in \\RR^{n \\times m}$ (addition and square term are element-wise) contains elements as kernel function between each pair of datapoints of X and Y,\n",
    "$$K_{ij} = \\phi(\\mathbf{x_i})^T \\phi(\\mathbf{y_j}) = k(\\mathbf{x_i}, \\mathbf{y_j}).$$\n",
    "\n",
    "### Kernelised Regression\n",
    "**TODO**:\n",
    "Write the function ```kernel_quadratic``` which takes $X$, $Y$  as input and $K$ as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def kernel_quadratic(X, Y):\n",
    "    dot_product = np.dot(X, Y.T)\n",
    "    return (dot_product+1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "Complete the function ```kernelised_lr``` with raw data matrix $X$ as input and root mean squared error (RMSE) for the training set and the test set as output. Inside of the function, make use of ```kernel_quadratic``` to apply dual representation, and calculate the predicted labels for training data and testing data repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def kernelised_lr(X, l, split_rate):\n",
    "    \"\"\"Return RMSE for the training set and the test set\n",
    "    \n",
    "    Parameters\n",
    "    ------------------------------------------------------\n",
    "    X: numpy matrix, whole dataset \n",
    "    l: float, regularisation parameter\n",
    "    split_rate: int, the percent of training dataset\n",
    "    \n",
    "    Returns\n",
    "    ------------------------------------------------------\n",
    "    rmse_train: float, RMSE for training set\n",
    "    rmse_test: float, RMSE for testing set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, t_train, X_test, t_test = split_data(X, split_rate)\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test = X_test.shape[0]\n",
    "    \n",
    "    K_train = kernel_quadratic(X_train, X_train)\n",
    "    K_test = kernel_quadratic(X_test, X_train)\n",
    "    \n",
    "    # critical points\n",
    "    a =  np.linalg.inv(K_train + l * np.eye(N_train)).dot(t_train)\n",
    "    \n",
    "    # Training set error\n",
    "    t_train_pred = np.dot(K_train, a)\n",
    "    rmse_train = np.linalg.norm(t_train_pred - t_train) / np.sqrt(N_train)\n",
    "\n",
    "    # Test set error\n",
    "    t_test_pred = np.dot(K_test, a)\n",
    "    rmse_test = np.linalg.norm(t_test_pred - t_test) / np.sqrt(N_test)\n",
    "\n",
    "    return rmse_train, rmse_test\n",
    "\n",
    "train_rmse, test_rmse = kernelised_lr(data, 1.1, 0.8)\n",
    "print(\"Kernelised Regression: RMSE with regularisation: Train {:.6f}, Test {:.6f}\".format(train_rmse, test_rmse))\n",
    "print(\"Expected value: Train 2.79, Test 3.33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time complexity\n",
    "\n",
    "Compare the above two methods (quadratic basis function, kernel method) in terms of time complexity.\n",
    "\n",
    "In terms of time complexity, is using a kernel method suitable in this case? What are potential advantages of using a kernel method? Disadvantages? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Assume the dataset with feature mapping is $N \\times M$. \n",
    "\n",
    "The linear regression with quadratic basis function requires to invert an $M \\times M$ matrix, while the linear regression with kernel method requires to invert a $N \\times N$ matrix. In our case, the feature mapping space is 91 (which is 12 + (12 * 11)/2 + 12 + 1), which is much smaller than the number of data points (506 in total). Thus a kernel method might not suit our case in terms of time complexity, but when N is smaller than M, kernel method is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook Questions (Optional)\n",
    "These questions are hand picked to both be of reasonable difficulty and demonstrate what you are expected to be able to solve. The questions are labelled in Bishop as either $\\star$, $\\star\\star$, or $\\star\\star\\star$ to rate its difficulty.\n",
    "\n",
    "- **Question 6.1**: Understand dual formulation (Difficulty $\\star\\star$, simple algebraic derivation)\n",
    "- **Question 6.2**: Dual formulation for perceptron learning algorithm (Difficulty $\\star\\star$, simple algebraic derivation)\n",
    "- **Question 6.11**: You may want to use Taylor expansion to represent term $\\exp(\\mathbf{x}^T\\mathbf{x}'/2\\sigma^2)$ (Difficulty $\\star$)\n",
    "- **Question 6.12**: To prove $k\\left(A_{1}, A_{2}\\right)=2^{\\left|A_{1} \\cap A_{2}\\right|}=\\phi\\left(A_{1}\\right)^{T} \\phi\\left(A_{2}\\right)$. (Difficulty $\\star\\star$)\n",
    "- **Question 6.13**: Use chain rule to represent $g(\\varphi(\\theta), x)$ (Difficulty $\\star$)\n",
    "- **Question 7.6**: (Difficulty $\\star$, simple algebraic derivation)\n",
    "- **Question 7.7**: (Difficulty $\\star$, simple algebraic derivation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
