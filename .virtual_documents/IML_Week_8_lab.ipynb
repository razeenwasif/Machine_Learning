


import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA





# set N, D
N = 200
D = 3

# generate data
X = np.random.randn(N, D)  # step 1 (What does randn do?)
t = np.random.randn(D, 1)  # step 2
Y = X @ t # step 3
theta = np.random.randn(D, 1)  # step 4

# gradient descent 
epoch = 300 # number of steps
lr = 0.01 # learning rate
loss_trace = []
for _ in range(epoch):
    # for record
    loss = (Y - X @ theta).T @ (Y - X @ theta) / N
    loss_trace.append(loss.item())
    # update
    gradient = (-2 * Y.T @ X + 2 * theta.T @ X.T @ X) / N
    theta = theta - lr * gradient.T

# plot
plt.plot(loss_trace)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

# check
theta_closed_form = np.linalg.inv(X.T @ X) @ X.T @ Y
print("Analytic Solution:\n", theta_closed_form)
print("Gradient Descent Solution:\n", theta)
print("t:\n", t)






#Create a reusable function
def gradient_descent(X, Y, epoch, lr):
    N = X.shape[0]
    theta = None#Your code goes here
    loss = []
    for _ in range(epoch):
        loss.append([])#Your code goes here
        theta = []#Your code goes here
    return theta, loss


# noise
N = 200
D = 3
X = np.random.randn(N, D)
t = np.random.randn(D, 1)
Y = X @ t
Y_tilde = X @ t + np.random.randn(N,1) #Noisy. What is the noise here?

epoch = 0 #Set a value here
lr = 0 #Set a value here

theta_1, _ = gradient_descent(X, Y, epoch=epoch, lr=lr) # without noise
theta_2, _ = gradient_descent(X, Y_tilde, epoch=epoch, lr=lr) # with noise

if theta_1 and theta_2 is not None:
    a = theta_1 - t
    b = theta_2 - t
else:
    a,b = theta_1,theta_2

print("Euclidean Distance:")
print("without noise\t", np.linalg.norm(a))
print("noise\t\t", np.linalg.norm(b))
print("Gradient Descent Solution")
print("without noise：\n", theta_1)
print("with noise：\n", theta_2)
print("t:\n", t)


# learning rate
N = 100
D = 3
X = np.random.randn(N, D)
Y = X @ np.random.randn(D, 1)

# training
#Try with different lr = 1, 10^-1, 10^-2, 10^-3...
loss_record = {}
for i in range(4):# your can try more
    lr = 0#Set your value here
    _, loss = gradient_descent(X, Y, epoch=300, lr=lr)
    loss_record[str(lr)] = np.array(loss)
    
# plot
fig=plt.figure(figsize=(15,5))
for i in []:#Plot for all your lr values
    lr = 0#Set your value here
    plt.plot(loss_record[str(lr)], label='lr='+str(lr))
plt.xlabel('epoch')
plt.ylabel('loss')
plt.ylim([0,10])
plt.legend()
plt.show()





X = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T
plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal');





pca = PCA(n_components=2) #Try calling scikitlearn PCA with 2 components
pca.fit(X)


print("Components:", pca.components_)
print("Variances:", pca.explained_variance_) #What are these properties? Explore the pca object


plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
for length, vector in zip(pca.explained_variance_, pca.components_):
    plt.gca().annotate('', pca.mean_, pca.mean_ + vector * 3 * np.sqrt(length), arrowprops=dict(arrowstyle='<-'))
plt.axis('equal');



