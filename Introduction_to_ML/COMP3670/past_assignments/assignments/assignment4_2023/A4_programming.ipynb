{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP3670/6670 Programming Assignment 4 - Gaussian mixture models ##\n",
    "---\n",
    "\n",
    "**Enter Your Student ID:**\n",
    "\n",
    "**Your Name:**\n",
    "    \n",
    "**Deadline:** Monday 23 October 23:59pm\n",
    "\n",
    "**Submit:** Write your answers in this file, and submit a single Jupyter Notebook file (.ipynb) on Wattle. Rename this file with your student number as 'uXXXXXXX.ipynb'. Note: you don't need to submit the .png or .npy files. \n",
    "\n",
    "**Enter Discussion Partner IDs Below:**\n",
    "Please implement things by yourself. If you use any external resources, list them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 0: Warming Up** ###\n",
    "\n",
    "We first load and visualise the data, initialise the Gaussian mixture components randomly and visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x = np.loadtxt(\"./data/gmm_data.txt\")\n",
    "N, D = x.shape[0], x.shape[1]\n",
    "\n",
    "print(\"number of data points %d\" % N)\n",
    "print(\"number of dimensions %d\" % D)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x[:, 0], x[:, 1], color='k', alpha=0.6)\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_mixture(X, mus, Sigmas, suffix=''):\n",
    "    # code to visualise the Gaussian components\n",
    "    # X is data, N x D. N: number of datapoints, D: dimensions\n",
    "    # mus contains means, K x D, K: number of Gaussian components\n",
    "    # Sigmas contains covariances, K x D x D.\n",
    "    \n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "    K = mus.shape[0]\n",
    "    for k in range(K):\n",
    "        m = mus[k, :]\n",
    "        cov = Sigmas[k, :, :]\n",
    "        lambda_, v = np.linalg.eig(cov)\n",
    "        lambda_ = np.sqrt(lambda_)\n",
    "        from matplotlib.patches import Ellipse\n",
    "        for j in range(1, 4):\n",
    "            ell = Ellipse(xy=m,\n",
    "                        width=lambda_[0]*j*2, height=lambda_[1]*j*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])))\n",
    "            ell.set_facecolor('none')\n",
    "            ell.set_edgecolor(CB_color_cycle[k])\n",
    "            ax.add_artist(ell)\n",
    "    plt.scatter(x[:, 0], x[:, 1], color='k', alpha=0.6)\n",
    "    plt.xlabel(r'$x_1$')\n",
    "    plt.ylabel(r'$x_2$')\n",
    "    plt.xlim([-8, 8])\n",
    "    plt.ylim([-8, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the mixture components randomly \n",
    "K = 2 # 2 components\n",
    "init_pis = np.random.rand(K) \n",
    "init_pis = init_pis / np.sum(init_pis)\n",
    "init_mus = np.random.uniform(-5, 5, K * D).reshape(K, D)\n",
    "init_Sigmas = np.array([np.eye(D) for _ in range(K)])\n",
    "\n",
    "# visualise\n",
    "viz_mixture(x, init_mus, init_Sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1: Computing GMM log-likelihood** ###\n",
    "\n",
    "The GMM log-likelihood for N data points is:\n",
    "$$\n",
    "  \\log p(x | \\theta) = \\sum_{n=1}^N \\log \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_n; \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Write a function to compute the log-likelihood of the GMMs parameters given data. \n",
    "\n",
    "Hint: have a look at Week 9's tutorial and the `compute_responsibility` function in Task 2 below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_log_likelihood(x, mus, Sigmas, pis):\n",
    "    # x is data, N x D. N: number of datapoints, D: dimensions\n",
    "    # mus contains means, K x D, K: number of Gaussian components.\n",
    "    # Sigmas contains covariances, K x D x D.\n",
    "    # pis contains the mixture weights, of size K.\n",
    "    # this function should return a number\n",
    "    \n",
    "    # your code here\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute the log likelihood of the initial parameters\n",
    "loglik = gmm_log_likelihood(x, init_mus, init_Sigmas, init_pis)\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2: Estimate GMM parameters** ###\n",
    "\n",
    "In Week 9's tutorial, you have implemented the E-step of the EM algorithm for GMMs to compure the assignment probability (see `compute_responsibility` below) . Your next task is to implement the updates required in the M-step.\n",
    "$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r_{nk} x_n, \\quad N_k = \\sum_{n=1}^N r_{nk}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k} \\sum_{n=1}^Nr_{nk}(x_n - \\mu_k)(x_n-\\mu_k)^\\intercal\n",
    "$$\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "Note that we are not asking you to handle the singularity issue in your code, just implement the updates above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_responsibility(x, mus, Sigmas, pis):\n",
    "    logpdfs = np.array([multivariate_normal.logpdf(x, mean=mus[k, :], cov=Sigmas[k, :, :]) for k in range(K)])\n",
    "    logpi = np.log(pis)\n",
    "    logpin = logpi[:, None] + logpdfs\n",
    "    denom = logsumexp(logpin, axis=0)\n",
    "    logres = logpin - denom[None, :]\n",
    "    res = np.exp(logres).T\n",
    "    return res\n",
    "    \n",
    "def update_mean(x, r):\n",
    "    # x is data, N x D\n",
    "    # r is responsibility, N x K\n",
    "    \n",
    "    # This function should return the new means, K x D\n",
    "    # your code here\n",
    "\n",
    "def update_cov(x, r, mus):\n",
    "    # x is data, N x D\n",
    "    # r is responsibility, N x K\n",
    "    # mus is the current means, K x D\n",
    "    \n",
    "    # This function should return the new covariances, K x D x D\n",
    "    # your code here\n",
    "\n",
    "def update_pi(r):\n",
    "    # r is responsibility, N x K\n",
    "    # This function should return the new mixture weights, of length K\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run a few EM iterations and and visualise the resulting mixture. Since we are not handling issues such as singularity, the following code block may fail. If that happens, try to reinitialise using the final code block in task 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_iters = 100\n",
    "mus, Sigmas, pis = init_mus, init_Sigmas, init_pis\n",
    "for i in range(no_iters):\n",
    "    r = compute_responsibility(x, mus, Sigmas, pis)\n",
    "    mus = update_mean(x, r)\n",
    "    Sigmas = update_cov(x, r, mus)\n",
    "    pis = update_pi(r)\n",
    "viz_mixture(x, mus, Sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3: Understanding the likelihood** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**\n",
    "\n",
    "In Task 1, you have computed the likelihood of the initial parameters. We can also compute the likelihood of the parameters given by EM. Compare the two likelihoods and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglik = gmm_log_likelihood(x, mus, Sigmas, pis)\n",
    "print(loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer goes here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**\n",
    "\n",
    "We can also evaluate the log likelihood given new test points. We will next visualise the log-likelihoods computed on training data and validation data when varying the number of mixture components. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = np.loadtxt(\"./data/gmm_data_valid.txt\")\n",
    "Ks = [1, 2, 4, 6, 8, 10, 12]\n",
    "no_iters = 100\n",
    "lik_train = []\n",
    "lik_valid = []\n",
    "for K in Ks:\n",
    "    while True:\n",
    "        try:\n",
    "            init_pis = np.random.rand(K) \n",
    "            init_pis = init_pis / np.sum(init_pis)\n",
    "            init_mus = np.random.uniform(-5, 5, K * D).reshape(K, D)\n",
    "            init_Sigmas = np.array([np.eye(D) for _ in range(K)])\n",
    "            mus, Sigmas, pis = init_mus, init_Sigmas, init_pis\n",
    "            for i in range(no_iters):\n",
    "                r = compute_responsibility(x, mus, Sigmas, pis)\n",
    "                mus = update_mean(x, r)\n",
    "                Sigmas = update_cov(x, r, mus)\n",
    "                pis = update_pi(r)\n",
    "            lik_train.append(gmm_log_likelihood(x, mus, Sigmas, pis))\n",
    "            lik_valid.append(gmm_log_likelihood(x_valid, mus, Sigmas, pis))\n",
    "            break\n",
    "        except:\n",
    "            # We just have another random restart\n",
    "            continue\n",
    "plt.figure()\n",
    "plt.plot(Ks, lik_train, color='r', lw=3, label='train')\n",
    "plt.plot(Ks, lik_valid, color='b', lw=3, label='valid')\n",
    "plt.legend()\n",
    "plt.xlabel('number of components, K')\n",
    "plt.ylabel('log-likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your comment goes here!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
