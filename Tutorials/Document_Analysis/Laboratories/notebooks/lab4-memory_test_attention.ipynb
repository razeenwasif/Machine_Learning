{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(dim_input, dim_hidden, 1, nonlinearity='relu')\n",
    "        self.W = nn.Linear(dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_all, h_final = self.rnn(x)\n",
    "        return self.W(h_final.squeeze(0))\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(dim_input, dim_hidden, 1)\n",
    "        self.W = nn.Linear(dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_all, h_final = self.gru(x)\n",
    "        return self.W(h_final.squeeze(0))\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(dim_input, dim_hidden, 1)\n",
    "        self.W = nn.Linear(dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_all, (h_final, c_final) = self.lstm(x)\n",
    "        return self.W(h_final.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSequenceDataset():\n",
    "    def __init__(self, dimension, sequence_length, batch_size, variance=1.0):\n",
    "        self.dim = dimension\n",
    "        self.sl = sequence_length\n",
    "        self.bs = batch_size\n",
    "        self.var = variance\n",
    "\n",
    "    def get_batch(self):\n",
    "        return torch.randn((self.sl, self.bs, self.dim)) * self.var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Modify **AttentivePooling** and **AttentionGRU**\n",
    "1) Implement different types of attention score functions.\n",
    "2) Implement the ReLU normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentivePooling(nn.Module):\n",
    "    # Computes attention with the last hidden state as the key\n",
    "    # Note: will calculate attention for all elements in the batch in parallel.\n",
    "\n",
    "    def __init__(self, dim_in):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim_in, dim_in)\n",
    "\n",
    "    def forward(self, h_all, xin):\n",
    "        # NOTE: h_all and xin both have the shape: [sequence length, batch size, hidden size]        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # get the last hidden state and compute a key from it \n",
    "        h_last = h_all[-1] # h_last is of shape [batch_size, hidden size]\n",
    "        key_a = self.W(h_last) # key_a is of shape [batch_size, hidden size]\n",
    "\n",
    "        # calculate the attention vector:\n",
    "        # permute dimensions in h_all to order: [batch, seq, hidden]\n",
    "        h_all = torch.permute(h_all, (1, 0, 2)) \n",
    "\n",
    "        # batch matrix multiplication of [batch, seq, hidden] x [batch, hidden, 1] = [batch, seq, 1]\n",
    "        a = torch.matmul(h_all, key_a.unsqueeze(2))\n",
    "\n",
    "        # remove the trailing dimension of a and then compute the softmax over the sequence dimension\n",
    "        a = nn.functional.softmax(a.squeeze(2), dim=-1)\n",
    "\n",
    "        # calculate the context vector using the attention and hidden states\n",
    "        # [batch, 1, seq] x [batch, seq, hidden] = [batch, 1, hidden]\n",
    "        output = torch.bmm(a.unsqueeze(1), h_all)\n",
    "\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGRU(nn.Module):\n",
    "    # A GRU with attention before a linear classification layer\n",
    "    # this is an attention GRU for classification, attention is only calculated\n",
    "    # at the end of the sequence rather than every step which is done in \n",
    "    # self-attention or attention auto-regressive models.\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(dim_input, dim_hidden, 1)\n",
    "        self.W = nn.Linear(dim_hidden, dim_output)\n",
    "        self.attention = AttentivePooling(dim_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_all, h_final = self.gru(x)\n",
    "        attn_output = self.attention(h_all, x)\n",
    "        res = self.W(attn_output)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = 4\n",
    "hidden_dimension = 4\n",
    "sequence_lengths = [3, 5, 10, 20, 40, 80, 100]\n",
    "\n",
    "# you can try this but the cpu is recommended for this task\n",
    "# it is often faster when the models are small like this\n",
    "device = 'cpu'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, sequence_length, dimension=input_dimension, batch_size=512):\n",
    "    # random dataset generator\n",
    "    dataset = RandomSequenceDataset(dimension, sequence_length, batch_size)\n",
    "\n",
    "    num_epochs = 100\n",
    "    num_batches = 5  # batches per epoch\n",
    "    best_loss = 1e10 # stores the best average training loss in any epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        average_loss = 0\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            # get the training batch and move it to the gpu if one is found\n",
    "            # shape: [sequence_length x batch_size x dimension]\n",
    "            x = dataset.get_batch().to(device)\n",
    "\n",
    "            # the target is the first element in the sequence, shape [batch_size x dimension]\n",
    "            y = x[0]\n",
    "\n",
    "            # run the specified type of RNN model and get the output, shape [batch_size x dimension]\n",
    "            y_h = model(x)\n",
    "\n",
    "            # calculate loss and update network\n",
    "            loss = F.mse_loss(y_h, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute average loss for this epoch\n",
    "            average_loss += loss.item() / num_batches\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, sequence_lengths):\n",
    "    plt.plot(sequence_lengths, losses, marker='o')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.ylim([-0.05, None])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_test(model_func, sequence_lengths):\n",
    "    # train the model on different length sequences \n",
    "    # and record the average training loss\n",
    "    losses = []\n",
    "    for sequence_length in sequence_lengths:\n",
    "        # create a new copy of the model\n",
    "        model = model_func().to(device)\n",
    "\n",
    "        # create an optimizer for the model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "        # train the model and store the best average loss\n",
    "        loss = train_model(model, optimizer, sequence_length=sequence_length)\n",
    "        print(f'Sequence length: {sequence_length}, \\t running average loss: {loss}')\n",
    "        losses.append(loss)\n",
    "    \n",
    "    plot_losses(losses, sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory_test(lambda: RNN(input_dimension, hidden_dimension, input_dimension), sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory_test(lambda: GRU(input_dimension, hidden_dimension, input_dimension), sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_test(lambda: LSTM(input_dimension, hidden_dimension, input_dimension), sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_test(lambda: AttentionGRU(input_dimension, hidden_dimension, input_dimension), sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "641fb509e98ff38cdeeb683417caf084b69c48eb4d5385ebfa679a29eb0d67b7"
  },
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
