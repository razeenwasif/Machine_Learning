{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange multipliers: PCA\n",
    "\n",
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial\n",
    "\n",
    "In this lab we will apply Lagrange multipliers to solve and implement principal component analysis (PCA)\n",
    "\n",
    "### Assumed knowledge:\n",
    "\n",
    "- Optimisation in Python (lab)\n",
    "- PCA (IML and lecture)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "\n",
    "- Applying Lagrange multipliers to optimisation problems\n",
    "- Implementing solutions with Lagrange multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "For this lab we will use a spam email dataset. This dataset contains 57 features which are word and character frequencies, as well as whether an email is spam or not. The last column of the dataset is the label, which indicates whether or not something is spam.\n",
    "\n",
    "Load and shuffle the data. Currently, the labels are an element of {0, 1}. Convert the labels into {-1 ,1} (as preparation for SVM). Then normalise the data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load the data into features and labels.\n",
    "data = np.genfromtxt('spambase.csv', delimiter=',',skip_header=1)\n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "features = np.nan_to_num(data[indices, :-1])\n",
    "# Convert binary labels into {-1, 1} (for the SVM later in the lab).\n",
    "labels = data[indices, -1].astype(bool) * 2 - 1\n",
    "\n",
    "# Normalise the data.\n",
    "features -= features.mean(axis=0, keepdims=True)\n",
    "features /= features.std(axis=0, keepdims=True)\n",
    "\n",
    "N = len(features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange multipliers\n",
    "\n",
    "One way to solve optimisation problems with equality constraints is to use Lagrange multipliers. We can write such optimisation problems in the following form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{x}{\\mathrm{maximise}}&\\ f(x)\\\\\n",
    "    \\mathrm{such\\ that}&\\ h(x) = 0.\n",
    "\\end{align*}\n",
    "\n",
    "We have an *objective function* $f : \\mathbb{R}^n \\to \\mathbb{R}$, a set of (vectorised) *equality constraints* $h : \\mathbb{R}^n \\to \\mathbb{R}^m$, and an *optimisation parameter* $x \\in \\mathbb{R}^n$.\n",
    "\n",
    "*Lagrange multipliers* are additional parameters $\\lambda \\in \\mathbb{R}^m$, which we introduce by defining the *Lagrange function* $\\mathcal L$ of this problem. The Lagrange function is\n",
    "\n",
    "$$\n",
    "    \\mathcal L(x, \\lambda) = f(x) + \\lambda \\cdot h(x).\n",
    "$$\n",
    "\n",
    "Note that for *equality* constraints, the \"+\" can be replaced with \"-\". For *inequality* constraints $g(x) \\le 0$, a \"+\" must be applied.\n",
    "\n",
    "Once we have a Lagrange function, we then need to find its stationary points, i.e. $\\nabla \\mathcal L_{x, \\lambda}(x^*, \\lambda^*) = 0$.\n",
    "\n",
    "*Also note that, $(x^*, \\lambda^*)$ only gives necessary conditions for optimality. In settings with convex $f(x)$ and affine $h(x)$, it is ususally sufficient to claim the sufficiency for the optimality of $(x^*, \\lambda^*)$. That is why in A1 we have to prove the convexity first.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal component analysis\n",
    "\n",
    "In PCA, we have $D$-dimensional observations $x_1, \\dots, x_N \\in \\mathbb{R}^D$. We want to find a 1-dimensional subspace with maximum variance. We can represent this subspace by a normal unit vector $u \\in \\mathbb R^D$.\n",
    "\n",
    "We can also write the set of observations in matrix form: $X \\in \\mathbb R^{N \\times D}$.\n",
    "\n",
    "Each observation is projected onto the subspace: for an observation $x$, the projected observation is $u^Tx$. The variance of the projected data is $u^TSu$ with covariance matrix $S = \\frac{1}{N} \\sum_{i = 1}^N \\left(x_i - \\bar x\\right) \\left(x_i - \\bar x\\right)^T$ where $\\bar x$ is the mean observation.\n",
    "\n",
    "We want to maximise the variance of observations projected onto $u$.\n",
    "\n",
    "(If necessary, refer to week 6 lab for more detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a.\n",
    "\n",
    "Write down the optimisation problem in the form shown above, clearly showing the objective function and equality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b.\n",
    "\n",
    "Write down the Lagrange function for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c.\n",
    "\n",
    "By differentiating the Lagrange function with respect to $u$ and $\\lambda$ and equating the derivatives to zero, show that the stationary points are unit eigenvectors of $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d.\n",
    "\n",
    "Find the stationary point that maximises the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e.\n",
    "\n",
    "In general, we can have a $k$-dimensional subspace. The $k$ principal components are the $k$ eigenvectors associated with the $k$ largest eigenvalues.\n",
    "\n",
    "Using `np.linalg`, find the first two principal components of the spam email features, project the data onto these components, then scatter plot these components with `matplotlib`. Colour the points in the plot by their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
