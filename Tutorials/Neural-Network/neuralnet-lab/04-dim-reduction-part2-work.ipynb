{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Statistical Machine Learning - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use dimensionality reduction techniques to explore a dataset of pictures.\n",
    "\n",
    "### Assumed knowledge\n",
    "- PCA (lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "- Implementing PCA\n",
    "- Visualising features derived from dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "For this lab, we will use a dataset of images of Pokemon sprites.\n",
    "\n",
    "Load the dataset from the file ``04-dataset.csv`` using ``np.loadtxt``. The datafile represents a 2d array where each row is a 64 by 64 pixel greyscale picture. The entries are floats between 0 and 1, where 0 is white and 1 is black.\n",
    "\n",
    "Note that while the images are 64 by 64 entries, the dataset you load has rows of size 4096 (which is $64\\times 64$) to allow the data to be saved as a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.loadtxt('04-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy dataset for debugging\n",
    "\n",
    "For debugging, it is useful to also have a simple dataset that we know is one-dimensional with some noise. You can use this to test your functions produce sensible output. Below is a function that generates data from two Gaussians in $\\mathbb{R}^n$ with unit variance, centered at $\\mathbf{1}$ and $-\\mathbf{1}$ respectively. (Note: $\\mathbf{1}$ is the vector $(1, 1, 1, ..., 1)$ in $\\mathbb{R}^n$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(n_samples=100, n_feat=5):\n",
    "    \"\"\"Generate data from two Gaussians\n",
    "    n_samples = number of samples from each Gaussian\n",
    "    n_feat = dimension of the features\n",
    "    \"\"\"\n",
    "    X1 = np.ones((n_feat, n_samples)) + np.random.randn(n_feat, n_samples)\n",
    "    X2 = -np.ones((n_feat, n_samples)) + np.random.randn(n_feat, n_samples)\n",
    "    X = np.hstack([X1,X2])\n",
    "    return X\n",
    "\n",
    "toy_data = gen_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA\n",
    "\n",
    "### Recap on PCA\n",
    "\n",
    "Remember from lectures that the goal of PCA is to linearly project data points onto a lower dimensional subspace such that the variance of the projected data is maximised. \n",
    "\n",
    "Let the data be the set of data points $\\{\\mathbf{x}_n\\}_{n=1}^N$, $\\mathbf{x}_n\\in\\mathbb{R}^d$, with mean $\\bar{\\mathbf{x}}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$ and covariance matrix $\\mathbf{S}=\\frac{1}{N}\\sum_{n=1}^N(\\mathbf{x}_n-\\bar{\\mathbf{x}})(\\mathbf{x}_n-\\bar{\\mathbf{x}})^T.$\n",
    "\n",
    "From lectures, we derived that if we are linearly projecting onto a subspace $m<d$, then the $m$ directions to linearly project on are given by the $m$ eigenvectors of $\\mathbf{S}$ whose eigenvalues are the $m$ largest, and the variance along each direction is equal to that eigenvalue.\n",
    "\n",
    "### Using the SVD to implement PCA\n",
    "\n",
    "Let us assume that $\\bar{\\mathbf{x}}=\\mathbf{0}$. Then $\\mathbf{S}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n\\mathbf{x}_n^T$. \n",
    "However, it turns out that\n",
    "$$\\sum_{n=1}^N\\mathbf{x}_n\\mathbf{x}_n^T=X^TX$$\n",
    "where $X\\in\\mathbb{R}^{N\\times d}$ is the data matrix.\n",
    "Thus to find the eigenvalues and vectors of the covariance matrix, we need to find the eigenvalues and vectors of $\\frac{1}{N}X^TX$.\n",
    "\n",
    "It turns out that if the SVD of $X$ is $X=U\\Sigma V^T$, then the eigenvectors of $\\mathbf{S}$ that correspond to its $k$ largest eigenvalues are the column vectors of $V$ that correspond to the $k$ largest singular values of $X$.\n",
    "\n",
    "### Question\n",
    "Show the left out steps (the two parts where it says \"it turns out\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement PCA\n",
    "Implement principal component analysis. Your function should take the data matrix and the number of components you wish to calculate and return two matrices:\n",
    "1. The projection of the data onto the principal components\n",
    "2. The actual components (eigenvectors) themselves.\n",
    "\n",
    "Hint: Do not forget to center the data by removing the mean so that you can use the above method. You may find ``np.linalg.svd`` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, n_pc=2):\n",
    "    \"\"\"Returns the projection onto the principal components (default=2)\"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    raise NotImplementedError\n",
    "\n",
    "def svd2pca(U,S,V, n_pc=2):\n",
    "    \"\"\"Returns the projection onto the principal components (default=2). Used for\n",
    "    when you want to change n_pc and have already computed the SVD once\"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the calculation with the toy data\n",
    "\n",
    "Below we calculate the projection of the toy data onto the first two principal components.\n",
    "\n",
    "1. Does PCA pick up the two Gaussians?\n",
    "2. What are the eigenvalues associated to these principal components? What do they tell you about how much variance these components explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_data.shape)\n",
    "Z, P, U, S, V = pca(toy_data.T)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(Z[:100,0], Z[:100,1], 'ro')\n",
    "ax.plot(Z[100:,0], Z[100:,1], 'bx')\n",
    "ax.set_xlabel('First principal component')\n",
    "ax.set_ylabel('Second principal component')\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen-pokemon\n",
    "\n",
    "If we perform PCA on a dataset, we expect the principal components to lie in the neighbourhood of our datapoints. In particular, if we do this on a dataset of images, we can interpret the principal components as images.\n",
    "\n",
    "The following function plots a gallery of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising images\n",
    "def plot_gallery(images, titles, h, w, n_row=2, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits.\n",
    "    Arguments: images: a matrix where each row is an image.\n",
    "    titles: an array of labels for each image.\n",
    "    h: the height in pixels of each image.\n",
    "    w: the width in pixels of each image.\n",
    "    n_row: the number of rows of images to print.\n",
    "    n_col: the number of columns of images to print.\"\"\"\n",
    "    assert len(images) >= n_row * n_col\n",
    "    assert len(titles) >= n_row * n_col\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``plot_gallery`` to plot the first 30 pokemon images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(images, np.arange(30), 64, 64, 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA on the Pokemon dataset to find the first 200 principal components. Visualise the first 100 using ``plot_gallery``.\n",
    "\n",
    "### Question\n",
    "\n",
    "What do you notice about the first few principal components? What are they detecting?\n",
    "Plot the associated eigenvalues. How can you interpret these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Answer</span>\n",
    "<i>--- replace this with your solution, add and remove code and markdown cells as appropriate ---</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing images using PCA\n",
    "\n",
    "Plot the reconstructions of the first 30 images using 200 principal components, and using the first 15 principal components. Don't forget to add the mean back in. How good are these reconstructions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your solution, add and remove code and markdown cells as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
