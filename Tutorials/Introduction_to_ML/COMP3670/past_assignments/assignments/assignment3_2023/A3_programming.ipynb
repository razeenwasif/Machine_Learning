{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP3670/6670 Programming Assignment 3 - Linear Regression ##\n",
    "---\n",
    "\n",
    "**Enter Your Student ID:**\n",
    "\n",
    "**Your Name:**\n",
    "    \n",
    "**Deadline:** \n",
    "\n",
    "**Submit:** Write your answers in this file, and submit a single Jupyter Notebook file (.ipynb) on Wattle. Rename this file with your student number as 'uXXXXXXX.ipynb'. Note: you don't need to submit the .png or .npy files. \n",
    "\n",
    "**Enter Discussion Partner IDs Below:**\n",
    "You could add more IDs with the same markdown format above.\n",
    "Please implement things by yourself. If you use any external resources, list them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section provides some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL FUNCTIONS - DO NOT MODIFY ##\n",
    "def lr_mle(X, y):\n",
    "    # maximum likelihood (least squares) for linear regression \n",
    "    XtX = np.dot(X.T, X)\n",
    "    Xty = np.dot(X.T, y)\n",
    "    theta = np.linalg.solve(XtX, Xty)\n",
    "    return theta\n",
    "\n",
    "def lr_map(X, y, alpha=0.1):\n",
    "    # maximum a-posteriori (regularised least squares) for linear regression\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    XtX = np.dot(X.T, X) + np.diag(alpha*N*np.ones(D))\n",
    "    Xty = np.dot(X.T, y)\n",
    "    theta = np.linalg.solve(XtX, Xty)\n",
    "    return theta\n",
    "\n",
    "def lr_bayes(X, y, alpha=0.1, noise_var=0.01):\n",
    "    # exact posterior for Bayesian linear regression\n",
    "    N, D = X.shape[0], X.shape[1]\n",
    "    XtX = np.dot(X.T, X) + np.diag(alpha*N*np.ones(D))\n",
    "    Xty = np.dot(X.T, y)\n",
    "    mean = np.linalg.solve(XtX, Xty)\n",
    "    # note: calling inv directly is not ideal\n",
    "    cov = np.linalg.inv(XtX) * noise_var\n",
    "    return mean, cov\n",
    "\n",
    "def predict_point(X, theta):\n",
    "    # predict given parameter estimate\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def predict_bayes(X, theta_mean, theta_cov):\n",
    "    # predict gien parameter posterior\n",
    "    mean = np.dot(X, theta_mean)\n",
    "    cov = np.dot(X, np.dot(theta_cov, X.T))\n",
    "    return mean, cov\n",
    "\n",
    "def add_bias_col(x):\n",
    "    # add an all-one column\n",
    "    n = x.shape[0]\n",
    "    return np.hstack([x, np.ones([n, 1])])\n",
    "\n",
    "## END GENERAL FUNCTIONS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 0: Warming Up** ###\n",
    "\n",
    "The following code block visualises the difference between different methods of performing linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.loadtxt(\"./data/ass3_data1_train.txt\")\n",
    "x_train, y_train = data[:, 0][:, None], data[:, 1][:, None]\n",
    "data = np.loadtxt(\"./data/ass3_data1_valid.txt\")\n",
    "x_valid, y_valid = data[:, 0][:, None], data[:, 1][:, None]\n",
    "\n",
    "# some data for visualisation\n",
    "N_plot = 100\n",
    "x_plot = np.linspace(-2.5, 2.5, N_plot).reshape([N_plot, 1])\n",
    "\n",
    "# add one col to the inputs\n",
    "x_train_with_bias = add_bias_col(x_train)\n",
    "x_plot_with_bias = add_bias_col(x_plot)\n",
    "\n",
    "# MLE = least squares\n",
    "theta_mle = lr_mle(x_train_with_bias, y_train)\n",
    "f_mle = predict_point(x_plot_with_bias, theta_mle)\n",
    "\n",
    "# MAP = regularised least squares\n",
    "alpha = 0.1\n",
    "theta_map = lr_map(x_train_with_bias, y_train, alpha)\n",
    "f_map = predict_point(x_plot_with_bias, theta_map)\n",
    "\n",
    "# exact Bayesian\n",
    "theta_mean, theta_cov = lr_bayes(x_train_with_bias, y_train, alpha)\n",
    "f_bayes_mean, f_bayes_cov = predict_bayes(\n",
    "    x_plot_with_bias, theta_mean, theta_cov)\n",
    "\n",
    "# plot utility\n",
    "def plot(x, y, x_plot, f_mle, f_map, f_bayes_mean, f_bayes_cov):\n",
    "    # plot utility\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(x, y, '+g', label='train data', ms=12)\n",
    "    if f_mle is not None:\n",
    "        plt.plot(x_plot, f_mle, '-k', label='mle')\n",
    "    if f_map is not None:\n",
    "        plt.plot(x_plot, f_map, '--k', label=\"map\", zorder=10)\n",
    "    if f_bayes_mean is not None:\n",
    "        plt.plot(x_plot, f_bayes_mean, '-r', label=\"bayes\", lw=3)\n",
    "        f_std = np.sqrt(np.diag(f_bayes_cov))\n",
    "        upper = f_bayes_mean[:, 0] + 2*f_std\n",
    "        lower = f_bayes_mean[:, 0] - 2*f_std\n",
    "        plt.fill_between(x_plot[:, 0], upper, lower, color='r', alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.ylim([-3, 3])\n",
    "\n",
    "# plot the training data and predictions\n",
    "plot(x_train, y_train, x_plot, f_mle, f_map, f_bayes_mean, f_bayes_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1: What makes a good regression?** ###\n",
    "\n",
    "As can be seen from the visualisation above, the regressed line seems to be far from the datapoints. Are there any ways we can improve the regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "\n",
    "Explain why the above linear regression fails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----Your answer here-----*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**\n",
    "\n",
    "What kind of features would lead to a better result? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----Your answer here-----*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1.3**\n",
    "\n",
    "Implement `featurise` function that takes raw datapoints as the input and output a reasonable design matrix $\\Phi$ according to the method you mentioned in **Task 1.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurise(x):\n",
    "    # TODO: Try to come up with proper features\n",
    "    features = add_bias_col(x) # change this!\n",
    "    return features\n",
    "\n",
    "x_train_feat = featurise(x_train)\n",
    "x_valid_feat = featurise(x_valid)\n",
    "x_plot_feat = featurise(x_plot) \n",
    "\n",
    "# repeat but now with features\n",
    "# MLE\n",
    "theta_mle = lr_mle(x_train_feat, y_train)\n",
    "f_mle = predict_point(x_plot_feat, theta_mle)\n",
    "\n",
    "# MAP\n",
    "alpha = 0.1\n",
    "theta_map = lr_map(x_train_feat, y_train, alpha)\n",
    "f_map = predict_point(x_plot_feat, theta_map)\n",
    "\n",
    "# exact Bayesian\n",
    "theta_mean, theta_cov = lr_bayes(x_train_feat, y_train, alpha)\n",
    "f_bayes_mean, f_bayes_cov = predict_bayes(\n",
    "    x_plot_feat, theta_mean, theta_cov)\n",
    "\n",
    "plot(x_train, y_train, x_plot, f_mle, f_map, f_bayes_mean, f_bayes_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2: Estimating noise variance through the marginal likelihood** ###\n",
    "\n",
    "One commonly asked question in Bayesian linear regression is how can we define the noise level of the target. In previous questions, we set the noise variance in `lr_bayes` to be 0.01 - a fixed constant. But intuitively, after we have observed some datapoints, the noise level can actually be inferred or estimated. This tasks is designed for you to investigate the marginal likelihood (a.k.a. model evidence) and how we can use this to pick the noise variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "\n",
    "Implement the negative log marginal likelihood, given the noise level of the likelihood, training inputs and outputs, and the prior variance. We can pick `prior_var` using the same procedure, but assume `prior_var = 0.5` for this exercise. The form of the marginal likelihood is provided in Week 7's lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "def negative_log_marginal_likelihood(noise_var, x, y, prior_var=0.5):\n",
    "    # TODO: implement this\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**\n",
    "\n",
    "Select the most appropriate noise level that minimises the negative log marginal likelihood. In practice, we can do this mimimisation by gradient descent, but for this exercise, we assume we have access to a predefined set of potential noise levels and just need to pick one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# a predefined list\n",
    "potential_noise_vars = np.logspace(-4, 1.5, 50)\n",
    "\n",
    "## YOUR CODE HERE! ##\n",
    "noise_var_estimated = potential_noise_vars[0] # change this!\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3**\n",
    "\n",
    "We visualise the predictions using the estimated noise variance, and compare to those when the noise is very large or very small. Based on these graphs and the negative log marginal likelihood corresponding to these noise levels, explain why finding a proper noise level of the likelihood is important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with the estimated noise variance\n",
    "N = x_train_feat.shape[0]\n",
    "prior_var = 0.5\n",
    "alpha = noise_var_estimated / prior_var / N\n",
    "theta_mean, theta_cov = lr_bayes(x_train_feat, y_train, alpha, noise_var_estimated)\n",
    "f_bayes_mean, f_bayes_cov = predict_bayes(\n",
    "    x_plot_feat, theta_mean, theta_cov)\n",
    "plot(x_train, y_train, x_plot, None, None, f_bayes_mean, f_bayes_cov)\n",
    "\n",
    "# fit with a very large noise\n",
    "noise_var = 5\n",
    "alpha = noise_var / prior_var / N\n",
    "theta_mean, theta_cov = lr_bayes(x_train_feat, y_train, alpha, noise_var)\n",
    "f_bayes_mean, f_bayes_cov = predict_bayes(\n",
    "    x_plot_feat, theta_mean, theta_cov)\n",
    "plot(x_train, y_train, x_plot, None, None, f_bayes_mean, f_bayes_cov)\n",
    "\n",
    "# fit with a very small noise\n",
    "noise_var = 0.00001\n",
    "alpha = noise_var / prior_var / N\n",
    "theta_mean, theta_cov = lr_bayes(x_train_feat, y_train, alpha, noise_var)\n",
    "f_bayes_mean, f_bayes_cov = predict_bayes(\n",
    "    x_plot_feat, theta_mean, theta_cov)\n",
    "plot(x_train, y_train, x_plot, None, None, f_bayes_mean, f_bayes_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----Your answer here-----*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2.4 - Optional **\n",
    "\n",
    "The naive implementation of the negative log marginal likelihood above would require the inverse of an N by N matrix, which is of time complexity $\\Theta(N^3)$. This is computationally intractable for a large dataset (large N). Can we speed this up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./data/ass3_data1_train_large.txt\")\n",
    "x_large, y_large = data[:, 0][:, None], data[:, 1][:, None]\n",
    "x_large_feat = featurise(x_large)\n",
    "\n",
    "def negative_log_marginal_likelihood_v2(noise_var, x, y, prior_var=0.5):\n",
    "    # TODO: implement this\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3: Regularisation** ###\n",
    "\n",
    "In machine learning, regularisation is an important technique to reduce overfitting. Regularisation also provides better generalisation in general. This task aims to show how regularisation affects the parameter estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**\n",
    "\n",
    "Implement `L1`, `L2`. Both functions take the weight $\\theta$ as input, output the regularisation value and the gradient of the regularisation term (**NOT THE GRADIENT OF THE ENTIRE OBJECTIVE FUNCTION**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(theta):\n",
    "    # TODO: implement this\n",
    "    return 0, np.zeros_like(theta) # change this\n",
    "\n",
    "def L2(theta):\n",
    "    # TODO: implement this\n",
    "    return 0, np.zeros_like(theta) # change this\n",
    "\n",
    "def data_fit(theta, x, y):\n",
    "    diff = y - np.dot(x, theta) # N x 1\n",
    "    f = np.mean(diff**2) # 1 x 1\n",
    "    df = - 2 * np.dot(diff.T, x).T / x.shape[0]\n",
    "    return f, df \n",
    "\n",
    "def objective(theta, x, y, alpha, l2=True):\n",
    "    reg_func = L2 if l2 else L1\n",
    "    reg, dreg = reg_func(theta)\n",
    "    fit, dfit = data_fit(theta, x, y)\n",
    "    obj = fit + alpha * reg\n",
    "    dobj = dfit + alpha * dreg\n",
    "    return obj, dobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2**\n",
    "\n",
    "We now run gradient descent and plot the predictions. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x_train_feat.shape[1]\n",
    "theta_l2_sgd_init = np.random.randn(D, 1)\n",
    "theta_l2_sgd = theta_l2_sgd_init\n",
    "\n",
    "no_iters = 2000\n",
    "learning_rate = 0.1\n",
    "alpha = 0.1\n",
    "l2 = True\n",
    "for i in range(no_iters):\n",
    "    obj, dobj = objective(theta_l2_sgd, x_train_feat, y_train, alpha, l2)\n",
    "    theta_l2_sgd -= learning_rate * dobj\n",
    "    if i % 100 == 0:\n",
    "        print(i, obj)\n",
    "\n",
    "f_l2_sgd = predict_point(x_plot_feat, theta_l2_sgd)\n",
    "\n",
    "theta_l1_sgd = theta_l2_sgd_init\n",
    "l2 = False\n",
    "for i in range(no_iters):\n",
    "    obj, dobj = objective(theta_l1_sgd, x_train_feat, y_train, alpha, l2)\n",
    "    theta_l1_sgd -= learning_rate * dobj\n",
    "    if i % 100 == 0:\n",
    "        print(i, obj)\n",
    "f_l1_sgd = predict_point(x_plot_feat, theta_l1_sgd)\n",
    "\n",
    "# Without any regularisation\n",
    "theta_noreg_sgd = theta_l2_sgd_init\n",
    "for i in range(no_iters):\n",
    "    obj, dobj = objective(theta_noreg_sgd, x_train_feat, y_train, 0, l2)\n",
    "    theta_noreg_sgd -= learning_rate * dobj\n",
    "    if i % 100 == 0:\n",
    "        print(i, obj)\n",
    "\n",
    "f_noreg_sgd = predict_point(x_plot_feat, theta_noreg_sgd)\n",
    "\n",
    "theta_map = lr_map(x_train_feat, y_train, alpha)\n",
    "f_map = predict_point(x_plot_feat, theta_map)\n",
    "\n",
    "# plot utility\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_train, y_train, '+g', label='train data', ms=12)\n",
    "plt.plot(x_plot, f_l2_sgd, '-k', lw=3, label='sgd l2')\n",
    "plt.plot(x_plot, f_l1_sgd, '-r', lw=2, label='sgd l1')\n",
    "plt.plot(x_plot, f_map, '--o', label=\"map\", zorder=10, lw=1)\n",
    "plt.plot(x_plot, f_noreg_sgd, '-x', label=\"no reg\", lw=2)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim([-3, 3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----Your answer here-----*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
